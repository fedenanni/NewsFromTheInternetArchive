{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import json, codecs,time,os\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "\n",
    "'''\n",
    "last part - for each article, it goes to the archived page and takes the content, then save it in a .tsv\n",
    "bear in mind a couple of things: the date is the archival date, which could differ from the published date\n",
    "if you want the published date you should scrape it from the content\n",
    "same thing for the labeled-topic of the article\n",
    "it will take a while\n",
    "'''\n",
    "\n",
    "# the name of the website as it appears in the URL, for example nytimes, guardian, lastampa\n",
    "website_name = \"nytimes\"\n",
    "\n",
    "# the precise homepage, from which you want to collect news (this step is needed as sometimes it could point to a subdomain, e.g. theguardian.com/uk)\n",
    "url_website = \"nytimes.com\"\n",
    "\n",
    "\n",
    "folder = \"Article-Repo/\"+website_name+\"/\"\n",
    "\n",
    "\n",
    "articles = codecs.open(website_name+\"_articles.txt\", \"r\", \"utf-8\").read().strip().split(\"\\n\")\n",
    "\n",
    "\n",
    "if not os.path.exists(\"Article-Repo/\"+website_name):\n",
    "    os.makedirs(\"Article-Repo/\"+website_name)\n",
    "\n",
    "\n",
    "for i in range(0,len(articles)):\n",
    "    article_link = articles[i]\n",
    "    try:\n",
    "        url_open = urllib.request.urlopen(article_link)\n",
    "                \n",
    "        soup = BeautifulSoup(url_open, \"lxml\")\n",
    "        title= soup.find('title')\n",
    "        \n",
    "        # lazy way of checking if you scraped a redirect page by mistake\n",
    "        if len(title)>0:\n",
    "            \n",
    "\n",
    "            output = codecs.open(\"Article-Repo/\"+website_name+\"/\"+str(i)+\".html\",\"w\",\"utf-8\")\n",
    "            \n",
    "            output.write(str(soup))\n",
    "            \n",
    "            output.close()\n",
    "            \n",
    "           # if (i % 10) == 0:\n",
    "            print (i+1, \" articles processed over \", len(articles))\n",
    "                #time.sleep(1)\n",
    "            \n",
    "    except Exception as e:\n",
    "        time.sleep(1)\n",
    "        pass\n",
    "\n",
    "output.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
