{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the first part of this script collects the homepages from the internet archive and save them in a .txt\n",
    "\n",
    "import urllib.request\n",
    "import json, codecs,time\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "\n",
    "# the name of the website as it appears in the URL, for example nytimes, guardian, lastampa\n",
    "website_name = \"guardian\"\n",
    "\n",
    "# the precise homepage, from which you want to collect news (this step is needed as sometimes it could point to a subdomain, e.g. theguardian.com/uk)\n",
    "url_website = \"theguardian.com/uk\"\n",
    "\n",
    "year = \"2016\"\n",
    "\n",
    "url = \"http://web.archive.org/cdx/search/cdx?url=\"+url_website+\"&matchType=url&&collapse=timestamp:8&limit=15000&filter=!mimetype:image/gif&filter=!mimetype:image/jpeg&from=\"+ year + \"01&to=\"+ year +\"12&output=json&limit=1\"\n",
    "\n",
    "http = urllib.request.urlopen(url)\n",
    "soup = http.read()                            \n",
    "http.close() \n",
    "\n",
    "ia_results = json.loads(soup)\n",
    "\n",
    "collected_dates = []\n",
    "\n",
    "homepages = []\n",
    "\n",
    "for i in range(1, len(ia_results)):\n",
    "    element = ia_results[i]\n",
    "    date = element[1][:8]\n",
    "    if date not in collected_dates:\n",
    "        collected_dates.append(date)\n",
    "        homepages.append(\"http://web.archive.org/web/\" + element[1] + \"/\" + element[2])\n",
    "print (len(homepages))\n",
    "\n",
    "output = codecs.open(website_name+\"_homepages.txt\", \"w\", \"utf-8\")\n",
    "\n",
    "for element in homepages:\n",
    "    output.write(element + \"\\n\")\n",
    "    \n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# the second part of this script extracts the links to the newspaper articles from each homepage and save them in a .txt\n",
    "# it will take quite a bit\n",
    "\n",
    "homepages = codecs.open(website_name+\"_homepages.txt\", \"r\", \"utf-8\").read().strip().split(\"\\n\")\n",
    "articles_link = codecs.open(website_name+\"_articles.txt\", \"w\", \"utf-8\")\n",
    "\n",
    "articles =  []\n",
    "\n",
    "for i in range(len(homepages)):\n",
    "    homepage = homepages[i]\n",
    "    try:\n",
    "        url_open = urllib.request.urlopen(homepage)\n",
    "        soup = BeautifulSoup(url_open, \"lxml\")\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            lk = link['href']\n",
    "            true_url = \"/\".join(lk.split(\"/\")[5:]).replace(\"https://\",\"\").replace(\"http://\",\"\")\n",
    "            if website_name in lk and true_url not in set(articles):\n",
    "                articles.append(true_url)\n",
    "                articles_link.write(lk+\"\\n\")\n",
    "    \n",
    "        print (i+1, \" homepages processed over \", len(homepages), \", \",len(articles),\" articles collected\")\n",
    "        time.sleep(1)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print (e)\n",
    "        time.sleep(5)\n",
    "        pass\n",
    "            \n",
    "\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "last part - for each article, it goes to the archived page and takes the content, then save it in a .tsv\n",
    "bear in mind a couple of things: the date is the archival date, which could differ from the published date\n",
    "if you want the published date you should scrape it from the content\n",
    "same thing for the labeled-topic of the article\n",
    "it will take a while\n",
    "'''\n",
    "\n",
    "articles = codecs.open(website_name+\"_articles.txt\", \"r\", \"utf-8\").read().strip().split(\"\\n\")\n",
    "\n",
    "output = codecs.open(website_name+\"_content-articles.tsv\",\"w\",\"utf-8\")\n",
    "\n",
    "\n",
    "for i in range(0,len(articles)):\n",
    "    article_link = articles[i]\n",
    "    try:\n",
    "        url_open = urllib.request.urlopen(article_link)\n",
    "                \n",
    "        soup = BeautifulSoup(url_open, \"lxml\")\n",
    "        title= soup.find('title')\n",
    "        \n",
    "        # lazy way of checking if you scraped a redirect page by mistake\n",
    "        if len(title)>0:\n",
    "            \n",
    "            title= \" \".join(soup.find('h1').text.split())\n",
    "            \n",
    "            date = article_link.split(\"/\")[4][:8]\n",
    "            \n",
    "            text = \"\"\n",
    "            \n",
    "            #ugly step - i'll improve it \n",
    "            soup = str(soup).split(\"<!-- END WAYBACK TOOLBAR INSERT -->\")[1]\n",
    "            \n",
    "            soup = BeautifulSoup(soup, \"lxml\")\n",
    "           \n",
    "            # simply take all the paragraphs\n",
    "            for link in soup.find_all('p'):\n",
    "                text+= link.text.replace(\"\\n\",\"\").replace(\"\\r\",\"\").replace(\"\\t\",\"\") + \" \"\n",
    "        \n",
    "            output.write(date +\"\\t\"+title+\"\\t\"+ text+\"\\n\")\n",
    "            print (i+1, \" homepages processed over \", len(homepages))\n",
    "            time.sleep(1)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print (e)\n",
    "        time.sleep(5)\n",
    "        pass\n",
    "\n",
    "output.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset = codecs.open(website_name+\"_content-articles.tsv\", \"r\", \"utf-8\").read().strip().split(\"\\n\")\n",
    "print (len(dataset))\n",
    "\n",
    "words = \" \".join([x.split(\"\\t\")[2] for x in dataset]).split(\" \")\n",
    "\n",
    "words = [x for x in words if len(x)>5]\n",
    "\n",
    "print (Counter(words).most_common(30))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
